<!DOCTYPE html>
<html lang="en">
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    
    <title>
        机器学习 - 01 人工神经网络 VS 生物神经网络 | Mao
    </title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
        <link rel="shortcut icon" href="/images/favicon.jpg">
    
    
<link rel="stylesheet" href="/css/style.css">

    <script id="hexo-configurations">
    let CONFIG = {"hostname":"wallxiaoming.github.io","root":"/","localsearch":{"enable":true,"trigger":"auto","unescape":false,"preload":false},"themeInfo":{"name":"ILS","version":"1.1.2","author":"XPoet","repository":"https://github.com/XPoet/hexo-theme-ils"},"path":"search.xml"};
  </script>
<meta name="generator" content="Hexo 5.0.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>


<body>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="page-template">
    <div class="page-top">
        <header class="header-wrapper">

    <div class="header-progress"></div>

    <div class="header-content">

        <a class="logo-title" href="/">
            Mao
        </a>

        <ul class="menu-list">
            
                <li class="menu-item">
                    <a class=""
                       href="/"
                    >
                        HOME
                    </a>
                </li>
            
                <li class="menu-item">
                    <a class=""
                       href="/archives"
                    >
                        ARCHIVES
                    </a>
                </li>
            
                <li class="menu-item">
                    <a class=""
                       href="/about"
                    >
                        ABOUT
                    </a>
                </li>
            
        </ul>

        <div class="menu-bar">
            <div class="menu-bar-middle"></div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item">
                    <a class=""
                       href="/">HOME</a>
                </li>
            
                <li class="drawer-menu-item">
                    <a class=""
                       href="/archives">ARCHIVES</a>
                </li>
            
                <li class="drawer-menu-item">
                    <a class=""
                       href="/about">ABOUT</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


    </div>

    <div class="page-middle ">

        <main class="main-content normal-code-theme">

            <div class="main-content-left">
                
                    <div class="fade-in-down-animation">
    <div class="article-content-container">

        <div class="article-title">
            <h3><a class="title-hover-animation">机器学习 - 01 人工神经网络 VS 生物神经网络</a></h3>
        </div>

        <div class="meta-info">
            <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa fa-calendar-o"></i> 2020-08-12 09:07:32
    </span>
    
        <span class="article-categories article-meta-item">
            <i class="fa fa-folder"></i>
            <ul>
                
                    <li>
                        <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
                    </li>
                
            </ul>
        </span>
    
    
        <span class="article-tags article-meta-item">
            <i class="fa fa-tags"></i>
            <ul>
                
                    <li>
                        <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
                    </li>
                
                    <li>
                        | <a href="/tags/pytorch/">pytorch</a>
                    </li>
                
            </ul>
        </span>
    
    
</div>
        </div>

        <div class="article-content markdown-body">
            <hr>
<h2 id="人工神经网络-VS-生物神经网络">&gt; 人工神经网络 VS 生物神经网络</h2>
<h3 id="Artificial-Neural-Nets-Vs-Neural-Nets">Artificial Neural Nets Vs Neural Nets</h3>
<p>相同与差别：</p>
<ul>
<li>都有神经元</li>
<li>但是人工神经网络是固定不变的。</li>
</ul>
<h3 id="在训练过程中">在训练过程中</h3>
<ul>
<li>生物神经网络会产生新的神经元，通过新的神经元来记忆信息</li>
<li>人工神经网络不会产生新的神经原，人工神经网络是预先准备大量正确数据，通过神经原的输出加工与正确数据进行对比，计算出loss值，在神经元中进行反向传递，从而修正神经元的强度，达到训练的目的。</li>
<li>即给定一个输入信号，和正确信息。输出信号通过神经元，到输出信号， 输出信号对比正确信息，计算loss梯度值， loss再进行反向传递，修改神经元强度，即修改神经元对正确信息有没有贡献</li>
</ul>
<h2 id="什么是神经网络（机器学习）">&gt; 什么是神经网络（机器学习）</h2>
<h3 id="人工神经网络是一种数学模型">人工神经网络是一种数学模型</h3>
<ul>
<li>神经网络是一种运算模型，由大量的节点（或称神经元）之间相互联接构成</li>
<li>每个节点代表一种特定的输出函数，称为激励函数（activation function）</li>
</ul>
<h2 id="神经网络梯度下降">&gt; 神经网络梯度下降</h2>
<h3 id="Gradient-Descent-In-Neural-Nets">Gradient Descent In Neural Nets</h3>
<ul>
<li>Optimization: Newton’s method, Least Squares method, Gradient Descent</li>
<li>Cost = (predicted - real)^2 = (Wx - y)^2 = (Wx - 0)^2</li>
</ul>
<h2 id="激励函数">&gt; 激励函数</h2>
<h3 id="Activation-Function">Activation Function</h3>
<ul>
<li>Linear and NonLinear</li>
<li>Y= Wx --&gt; y = AF(Wx)</li>
<li>AF : relu, sigmoid, tanh</li>
<li>当神经层只有两三层不是很多的时候任意的激励函数都行</li>
<li>卷积神经网络推荐的是relu</li>
</ul>
<h3 id="激励函数的使用">激励函数的使用</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># fake data</span></span><br><span class="line">x = torch.linspace(<span class="number">-5</span>, <span class="number">5</span>, <span class="number">200</span>)</span><br><span class="line">x = Variable(x)</span><br><span class="line">x_np = x.data.numpy()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">y_relu = torch.relu(x).data.numpy()</span><br><span class="line">y_sigmoid = torch.sigmoid(x).data.numpy()</span><br><span class="line">y_tanh = torch.tanh(x).data.numpy()</span><br><span class="line">y_softplus = F.softplus(x).data.numpy()</span><br><span class="line"><span class="comment"># y_sofymax = F.softmax()</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.figure(<span class="number">1</span>, figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">plt.subplot(<span class="number">221</span>)</span><br><span class="line">plt.plot(x_np, y_relu, c=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">plt.ylim((<span class="number">-1</span>, <span class="number">5</span>))</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">222</span>)</span><br><span class="line">plt.plot(x_np, y_sigmoid, c=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line">plt.ylim((<span class="number">-0.2</span>, <span class="number">1.2</span>))</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">223</span>)</span><br><span class="line">plt.plot(x_np, y_tanh, c=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;tanh&#x27;</span>)</span><br><span class="line">plt.ylim((<span class="number">-1.2</span>, <span class="number">1.2</span>))</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">224</span>)</span><br><span class="line">plt.plot(x_np, y_softplus, c=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;softplus&#x27;</span>)</span><br><span class="line">plt.ylim((<span class="number">-0.2</span>, <span class="number">6</span>))</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://blogmypic.oss-cn-beijing.aliyuncs.com/img/20200812085756.png" alt=""></p>
<ul>
<li>激励函数的使用：torch.tanh, torch.relu, torch.sigmoid, torch.nn.functional.softplus, torch.nn.functional.softmax</li>
</ul>
<h2 id="Regression-回归">&gt; Regression 回归</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># fake data 创建假数据</span></span><br><span class="line"></span><br><span class="line">x = torch.unsqueeze(torch.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">100</span>), dim=<span class="number">1</span>)</span><br><span class="line">y = x.pow(<span class="number">2</span>) + <span class="number">0.2</span> * torch.rand(x.size())</span><br><span class="line"></span><br><span class="line">x, y = Variable(x), Variable(y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plt.scatter(x.data.numpy(), y.data.numpy())</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_feature, n_hidden, n_output</span>):</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.hidden = torch.nn.Linear(n_feature, n_hidden) <span class="comment"># 层数的输入和输出</span></span><br><span class="line">        self.predict = torch.nn.Linear(n_hidden, n_output)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = torch.relu(self.hidden(x))</span><br><span class="line">        x = self.predict(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = Net(<span class="number">1</span>, <span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># print(net)</span></span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">loss_func = torch.nn.MSELoss()</span><br><span class="line"></span><br><span class="line">plt.ion()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    pre = net(x)</span><br><span class="line"></span><br><span class="line">    loss = loss_func(pre, y)</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># plot and show learning process</span></span><br><span class="line">        plt.cla()</span><br><span class="line">        plt.scatter(x.data.numpy(), y.data.numpy())</span><br><span class="line">        plt.plot(x.data.numpy(), pre.data.numpy(), <span class="string">&#x27;r-&#x27;</span>, lw=<span class="number">5</span>)</span><br><span class="line">        plt.text(<span class="number">0.5</span>, <span class="number">0</span>, <span class="string">&#x27;Loss=%.4f&#x27;</span> % loss.data.numpy(), fontdict=&#123;<span class="string">&#x27;size&#x27;</span>: <span class="number">20</span>, <span class="string">&#x27;color&#x27;</span>:  <span class="string">&#x27;red&#x27;</span>&#125;)</span><br><span class="line">        plt.pause(<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.ioff()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://blogmypic.oss-cn-beijing.aliyuncs.com/img/20200812085819.png" alt=""></p>
<p><img src="https://blogmypic.oss-cn-beijing.aliyuncs.com/img/20200812085828.png" alt=""></p>
<p><img src="https://blogmypic.oss-cn-beijing.aliyuncs.com/img/20200812085833.png" alt=""></p>
<p><img src="https://blogmypic.oss-cn-beijing.aliyuncs.com/img/20200812085842.png" alt=""></p>
<p><img src="https://blogmypic.oss-cn-beijing.aliyuncs.com/img/20200812085848.png" alt=""></p>
<p><img src="https://blogmypic.oss-cn-beijing.aliyuncs.com/img/20200812085856.png" alt=""></p>
<p><img src="https://blogmypic.oss-cn-beijing.aliyuncs.com/img/20200812085906.png" alt=""></p>

        </div>

        <div class="article-nav">
            
                <div class="article-prev">
                    <a class="prev btn"
                       rel="prev"
                       href="/2020/08/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%2002_Regression%E5%9B%9E%E5%BD%92/"
                    >
                        <i class="fa fa-chevron-left"></i> <span class="post-nav-title-item">机器学习 - 02 Regression 回归</span><span class="post-nav-item">Prev posts</span>
                    </a>
                </div>
            
            
                <div class="article-next">
                    <a class="next btn"
                       rel="next"
                       href="/2020/08/05/%E7%AE%97%E6%B3%95/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/%E7%AE%97%E6%B3%95%20-%20Morris%E9%81%8D%E5%8E%86/"
                    >
                        <span class="post-nav-title-item">算法 - Morris遍历</span><span class="post-nav-item">Next posts</span> <i class="fa fa-chevron-right"></i>
                    </a>
                </div>
            
        </div>

        <div class="comment-container">
            <div class="comments-container">
    
</div>
        </div>
    </div>
</div>

    <div class="article-toc-container fade-in-down-animation">
        <div class="article-toc">
            <div class="post-toc-wrap">
    <div class="post-toc">
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-VS-%E7%94%9F%E7%89%A9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">1.</span> <span class="nav-text">&gt; 人工神经网络 VS 生物神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Artificial-Neural-Nets-Vs-Neural-Nets"><span class="nav-number">1.1.</span> <span class="nav-text">Artificial Neural Nets Vs Neural Nets</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9C%A8%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E4%B8%AD"><span class="nav-number">1.2.</span> <span class="nav-text">在训练过程中</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%89"><span class="nav-number">2.</span> <span class="nav-text">&gt; 什么是神经网络（机器学习）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%98%AF%E4%B8%80%E7%A7%8D%E6%95%B0%E5%AD%A6%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.1.</span> <span class="nav-text">人工神经网络是一种数学模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">3.</span> <span class="nav-text">&gt; 神经网络梯度下降</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Gradient-Descent-In-Neural-Nets"><span class="nav-number">3.1.</span> <span class="nav-text">Gradient Descent In Neural Nets</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%BF%80%E5%8A%B1%E5%87%BD%E6%95%B0"><span class="nav-number">4.</span> <span class="nav-text">&gt; 激励函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Activation-Function"><span class="nav-number">4.1.</span> <span class="nav-text">Activation Function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%BF%80%E5%8A%B1%E5%87%BD%E6%95%B0%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="nav-number">4.2.</span> <span class="nav-text">激励函数的使用</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Regression-%E5%9B%9E%E5%BD%92"><span class="nav-number">5.</span> <span class="nav-text">&gt; Regression 回归</span></a></li></ol>
    </div>
</div>
        </div>
    </div>


                

            </div>

            

        </main>

        <div class="sidebar-tools">
            <div class="tools-container">
    <ul class="tools-list">
        
            <li class="search popup-trigger">
                <i class="fa fa-search"></i>
            </li>
            
<script src="/js/local-search.js"></script>

        
        <li class="mode-toggle">
            <i class="fa fa-moon-o"></i>
        </li>
        
    </ul>
</div>

        </div>

        
            <div class="scroll-to-top">
                <ul>
                    <li>
                        <!--<i class="fa fa-caret-up"></i>-->
                        <span class="scroll-percent"></span>
                    </li>
                </ul>
            </div>
        
    </div>

    <div class="page-bottom">
        <footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy; 2020 <a href="/">ILS</a>
        </div>


        <a href="http://www.beian.miit.gov.cn/"  target="_blank">蜀ICP备20024492号</a>




        
    </div>
</footer>

    </div>
</div>

    <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-icon">
            <i class="fa fa-search"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="Search..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fa fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>


<script src="/js/main.js"></script>
<script src="/js/header-shrink.js"></script>
<script src="/js/toggle-mode.js"></script>



    
<script src="/js/scroll-to-top.js"></script>





    
        
<script src="/js/code-copy.js"></script>

    

    
        
<script src="/lib/anime.min.js"></script>
<script src="/js/toc.js"></script>

    




<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>